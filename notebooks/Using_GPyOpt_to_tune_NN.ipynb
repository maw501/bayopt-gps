{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will walk through the MNIST example and show how to use GPyOpt to tune a NN in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from GPyOpt.methods import BayesianOptimization\n",
    "import GPyOpt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from src import data_prep\n",
    "from src import models\n",
    "from src import train\n",
    "from src import utils\n",
    "from src import plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The aim of this notebook is to provide a practical example of how to use GPyOpt for tuning a NN - not to motivate and introduce background concepts. However, as a minimum, it is helpful to be familiar with the following areas:\n",
    "\n",
    "* Gaussian processes\n",
    "* Acquisition functions\n",
    "* Kernels\n",
    "* Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters to optimize\n",
    "\n",
    "With GPyOpt we can basically optimize most types of parameters, including discrete and categorical. This allows great flexibility when fitting a NN though I can't find any strong references backing this up (particularly the categorical) and so I'm unsure how well it will work in practice.\n",
    "\n",
    "For MNIST, given it's a small and easy data-set and I'm using a GPU we will go for quite a large domain. A simpler case would be to fix the model architecture (e.g. use a pre-trained model) and just let GPyOpt tune the hyperparameters. It's worth noting that if we did grid search on 7 parameters (as we are below) and each of those parameters could take on just 4 values we'd have $4^7$ possible values to try. \n",
    "\n",
    "Here we are going to let the BO actually vary the model's capacity via `num_lay`, `num_c` and `num_fc` which control the number of layers in the CNN, the number of channels (doubled at each layer by default) and the size of the fully connected layer at the end.\n",
    "\n",
    "Below is an example of how we specify this to GPyOpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_BO = [\n",
    "    {\"name\": \"lr\", \"type\": \"continuous\", \"domain\": (0.05, 0.25)},\n",
    "    {\"name\": \"mom\", \"type\": \"continuous\", \"domain\": (0.85, 0.95)},\n",
    "    {\"name\": \"num_lay\", \"type\": \"discrete\", \"domain\": range(1, 4)},\n",
    "    {\"name\": \"num_c\", \"type\": \"discrete\", \"domain\": range(8, 22, 2)},\n",
    "    {\"name\": \"num_fc\", \"type\": \"discrete\", \"domain\": range(10, 105, 5)},\n",
    "    {\"name\": \"dropout\", \"type\": \"discrete\", \"domain\": np.linspace(0, 0.4, 11)},\n",
    "    {\"name\": \"bs\", \"type\": \"discrete\", \"domain\": range(64, 288, 32)},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Let's have a look at a simple CNN for MNIST. The model itself will be straightforward but we use a few neats aspects of PyTorch to allow us to vary the number of layers e.g. via `nn.ModuleList`\n",
    "\n",
    "We are using a `kernel_size=5` so with the default values of `padding=0` and `stride=1` the feature map will reduce by 4 each time we call `ConvBnRelu`. We also opt for a convention of doubling the number of channels at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FlexCNN(\n",
       "  (convs): ModuleList(\n",
       "    (0): ConvBnRelu(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout2d(p=0.1, inplace=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBnRelu(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout2d(p=0.1, inplace=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBnRelu(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): Dropout2d(p=0.1, inplace=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=16384, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.FlexCNN()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "It's actually fairly simple to get up and running with GPyOpt. The main function (and work) is to create an objective function that GPyOpt will either minimize or maximize.\n",
    "\n",
    "The function doesn't need to do the data-loading unless parameters we wish to vary are part of the data-loading process (e.g. batch size).\n",
    "\n",
    "We then instantiate and fit the model, passing in the parameters we wish to optimize over. The `train.fit` function takes a model and fits it for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.call_counter\n",
    "def f_opt_mnist(parameters):\n",
    "    parameters = parameters[0]  # np.ndarray passed in is nested\n",
    "    print(\n",
    "        f\"---------Starting Bay opt call {f_opt_mnist.calls} with parameters: ---------\"\n",
    "    )\n",
    "    utils.print_params(parameters, opt_BO)\n",
    "    # Data loading:\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root=\"../data\", train=True, download=True, transform=data_prep.mnist_transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=int(parameters[6]), shuffle=True, num_workers=8\n",
    "    )\n",
    "    testset = torchvision.datasets.MNIST(\n",
    "        root=\"../data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=data_prep.mnist_transform,\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=int(parameters[6]), shuffle=False, num_workers=8\n",
    "    )\n",
    "    train_dl, valid_dl = (\n",
    "        data_prep.WrapDL(trainloader, data_prep.to_gpu),\n",
    "        data_prep.WrapDL(testloader, data_prep.to_gpu),\n",
    "    )\n",
    "\n",
    "    # Model definition:\n",
    "    model = models.FlexCNN(\n",
    "        n_lay=int(parameters[2]),\n",
    "        n_c=int(parameters[3]),\n",
    "        n_fc=int(parameters[4]),\n",
    "        dropout=parameters[5],\n",
    "    )\n",
    "\n",
    "    # Optimizer:\n",
    "    opt = optim.SGD(\n",
    "        model.parameters(), lr=parameters[0], momentum=parameters[1]\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CyclicLR(\n",
    "        opt,\n",
    "        parameters[0],\n",
    "        parameters[0] * 3,\n",
    "        step_size_up=int(len(train_dl) * int(parameters[6]) * epochs * 0.25),\n",
    "    )\n",
    "    # Fit:\n",
    "    score = train.fit(\n",
    "        epochs,\n",
    "        model,\n",
    "        loss_func,\n",
    "        scheduler,\n",
    "        train_dl,\n",
    "        valid_dl,\n",
    "        train.accuracy,\n",
    "        model_folder,\n",
    "    )\n",
    "    return np.array(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to specify the loss function, as well as how many epochs and iterations to run the BO for, plus a few other general fitting settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = True\n",
    "loss_func = F.cross_entropy\n",
    "opt_func = f_opt_mnist\n",
    "cleanup_models_dir = (\n",
    "    True  # will delete models from model_folder before running\n",
    ")\n",
    "model_folder = \"../models/mnist/\"\n",
    "epochs = 4\n",
    "max_iter = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a folder to store the models in after each BO has finished - this means we don't have to potentially re-train a large NN again after finding the best parameters\n",
    "\n",
    "We are now ready to execute the main function call to `BayesianOptimization` and run for `max_iter` + 5 initial runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting models from: ../models/mnist/\n",
      "---------Starting Bay opt call 1 with parameters: ---------\n",
      "| lr: 0.21 | mom: 0.94 | num_lay: 3.00 | num_c: 10.00 | num_fc: 20.00 | dropout: 0.24 | bs: 96.00 |\n",
      "\n",
      "e: 1, trn loss: 2.3263, val loss: 2.3043, val met: 0.1135, in: 82 secs\n",
      "e: 2, trn loss: 2.3062, val loss: 2.3034, val met: 0.1032, in: 82 secs\n",
      "e: 3, trn loss: 1.8665, val loss: 0.5497, val met: 0.8144, in: 84 secs\n",
      "e: 4, trn loss: 0.5415, val loss: 0.1973, val met: 0.9450, in: 82 secs\n",
      "---------Starting Bay opt call 2 with parameters: ---------\n",
      "| lr: 0.15 | mom: 0.86 | num_lay: 3.00 | num_c: 14.00 | num_fc: 85.00 | dropout: 0.36 | bs: 96.00 |\n",
      "\n",
      "e: 1, trn loss: 0.7613, val loss: 0.1851, val met: 0.9446, in: 120 secs\n",
      "e: 2, trn loss: 0.2861, val loss: 0.1274, val met: 0.9627, in: 125 secs\n",
      "e: 3, trn loss: 0.2468, val loss: 0.1193, val met: 0.9651, in: 126 secs\n"
     ]
    }
   ],
   "source": [
    "utils.setup_folders(model_folder)  # create folders if don't exist\n",
    "if cleanup_models_dir:\n",
    "    utils.clean_folder(model_folder)  # delete models from previous runs\n",
    "\n",
    "# GPyOpt function call:\n",
    "optimizer = BayesianOptimization(\n",
    "    f=opt_func,\n",
    "    domain=opt_BO,\n",
    "    model_type=\"GP\",\n",
    "    acquisition_type=\"EI\",\n",
    "    normalize_Y=True,\n",
    "    acquisition_jitter=0.05,  # positive value to make acquisition more explorative\n",
    "    exact_feval=False,  # whether the outputs are exact\n",
    "    maximize=False,\n",
    ")\n",
    "optimizer.run_optimization(\n",
    "    max_iter=max_iter\n",
    ")  # 5 initial exploratory points + max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot:\n",
    "    plotting.plot_convergence(optimizer.X, optimizer.Y_best)\n",
    "print(\"--------------------------------------------------\")\n",
    "print(\"Optimal parameters from Bay opt are:\")\n",
    "utils.print_params(optimizer.x_opt, opt_BO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment\n",
    "\n",
    "#### Normal machine learning reporting of results\n",
    "Depending on randomness the above reaches a validation accuracy of around 99% in 4 epochs with just a few runs.\n",
    "\n",
    "#### What actually happened\n",
    "_\"It doesn't matter how beautiful your theory is, it doesn't matter how smart you are. If it doesn't agree with experiment, it's wrong.\"_\n",
    "\n",
    "_\"If science is to progress, what we need is the ability to experiment, honesty in reporting results—the results must be reported without somebody saying what they would like the results to have been—and finally—an important thing—the intelligence to interpret the results.\"_\n",
    "\n",
    "It would be misleading people to present the above as though it worked first time out the box. Many times training diverged on me almost as much as it converged. This was predominantly because I had a desire to leave the domains for most parameter settings reasonably wide so as not to constrain the search too much by embedding lots of prior knowledge (after all, MNIST is a pretty overfit dataset and many scripts are online showing over 99% accuracy). \n",
    "\n",
    "This is clearly a tension that existed in creating this example...Bayesian frameworks allow the expression of prior knowledge but by guiding things too much on MNIST I'm going to overestimate the usefulness of BO for a new data-set I know less about. \n",
    "\n",
    "If training on bigger datasets is several hours then wasted trials or divergence is a big issue*. \n",
    "\n",
    "So where does this leave us?\n",
    "\n",
    "Well, we haven't talked about tuning the BO (via the GP kernel or exploration/exploitation trade-off) at all yet and it's here we get to a central criticism of BO, to quote [A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning](http://arxiv.org/abs/1012.2599):\n",
    "\n",
    "\"_A particular issue is that the design of the prior is absolutely critical to efficient Bayesian optimization. Gaussian processes are not always the best or easiest solution, but even when they are, great care must be taken in the design of the kernel. In many cases, though, little is known about the objective function, and, of course, it is expensive to sample from (or we wouldn’t need to use Bayesian optimization in the first place). The practical result is that in the absence of (expensive) data, either strong assumptions are made without certainty that they hold, or a weak prior must be used. It is also often unclear how to handle the trade-off between exploration and exploitation in the acquisition function. Too much exploration, and many iterations can go by without improvement. Too much exploitation leads to local maximization._\n",
    "\n",
    "_These problems are exacerbated as dimensionality is increased—more dimensions means more samples are required to cover the space, and more parameters and hyperparameters may need to be tuned, as well. In order to deal with this problem effectively, it may be necessary to do automatic feature selection, or assume independence and optimize each dimension individually._\"\n",
    "\n",
    "#### TLDR\n",
    "\n",
    "In it's current guise BO is unlikely to be something that provides a silver bullet for tuning NNs in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\*One might counter than no trial is wasted as it's information for the GP to reduce its uncertainty and this is definitely true, however, too many poor trials can leave the experimenter wondering if the experiment itself is flawed rather than just the individual trial settings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bayopt]",
   "language": "python",
   "name": "conda-env-bayopt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
